{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================= Start of importing required packages and libraries =========================================#\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import *\n",
    "from matplotlib import pyplot as plt\n",
    "from dataset import Dataset\n",
    "from models import Net, CNNCifar\n",
    "import os\n",
    "from torch.utils import data\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "\n",
    "#================================== End of importing required packages and libraries ==========================================#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#=============================== Defining global variables ========================#\n",
    "\n",
    "#10 workers in this federated model \n",
    "num_of_workers = 10\n",
    "#10 10 in this federated model classification problem \n",
    "num_of_classes = 10\n",
    "#initialization of reputation values with 0.1 for each worker \n",
    "global_reputaion = [0.1 for i in range(num_of_workers)]\n",
    "\n",
    "\n",
    "#======================================= Fix the model´s seed to reproduce a deterministic results ============================#\n",
    "\n",
    "#=============================== Start of fixing the model´s seed to reproduce a deterministic results ========================#\n",
    "def seed_everything(seed=7):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything()\n",
    "#======================================= Fix the model´s seed to reproduce a deterministic results ============================#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#===================================== Start of classes, lables and workers lists and dictionaries ========================================#\n",
    "lables_dict = {\"airplane\":0, \n",
    "        \"automobile\":1,\n",
    "        \"bird\":2,\n",
    "        \"cat\":3,\n",
    "        \"deer\":4,\n",
    "        \"dog\":5,\n",
    "        \"frog\":6,\n",
    "        \"horse\":7,\n",
    "        \"ship\":8,\n",
    "        \"truck\":9}\n",
    "\n",
    "classes = ('plane', 'car',  'bird',  'cat',  'deer',\n",
    "                       'dog',   'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "workers = ['Josep', 'David', 'Oriol','Alberto', 'Jesus', 'Michel', 'Fadi', 'Rami', 'Ashneet', 'Najeeb']\n",
    "\n",
    "\n",
    "\n",
    "#Change the directory to the place of test dataset\n",
    "os.chdir(\"d:/Federated_Learning/Applications/cifar_classification_model/\")\n",
    "PATH = os.path.join(os.getcwd(), 'checkpoint')\n",
    "#======================================= End of classes and lables list and dictionary ========================================#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#======================================= Start of training function ===========================================================#\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #data = data.transpose(3, 1)\n",
    "        data=data.float()\n",
    "        \n",
    "        #poisoning attack by changing the decision about a dog to be a cat\n",
    "                \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        if batch_idx > 0 and batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{}\\t({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                (epoch+1), batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "        \n",
    "    return losses\n",
    "#======================================= End of training function =============================================================#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#======================================= Start of testning function ===========================================================#\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            #data = data.transpose(3, 1)\n",
    "            data=data.float()\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target)#, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return (float(correct) / len(test_loader.dataset))\n",
    "#======================================= End of testning function =============================================================#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#======================================= Start of worker´s task function and attack simulation======================================================#\n",
    "def worker_task(worker_id, epoch, model, device, data_loader, task_type,ask_for_local_reputation, \n",
    "                targeted_poisoning_attack = False, stealthy_poisoning_attack = False):\n",
    "    \n",
    "    #read local_reputations from csv or txt file \n",
    "    gradients = []\n",
    "    accuracy= 0.0\n",
    "    \n",
    "    if(task_type == 'train'):\n",
    "        return gradients\n",
    "    if(task_type == 'test'):\n",
    "        return accuracy\n",
    "    if(task_type == 'localreputation'):\n",
    "        localopinion = []\n",
    "        for worker in ask_for_local_reputation:\n",
    "            localopinion.append(local_reputation[worker])\n",
    "            return localopinion\n",
    "            \n",
    "#======================================= End of worker´s task function ========================================================#    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#======================================= Start of distribution and loading of the datasets ====================================#\n",
    "#paths to training and testing datasets\n",
    "train_path = str(os.getcwd()+\"/dataset/train/\")\n",
    "test_path = str(os.getcwd() + \"/dataset/test\")\n",
    "\n",
    "# Define a transforms to normalize and augment the data\n",
    "ransform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset_worker1 = Dataset(train_path, ransform_train, 0, 50000, 32, True)\n",
    "test_dataset_worker1 = Dataset(test_path, transform_test,  0, 10000, 32, True) \n",
    "\n",
    "\n",
    "len_of_training_samples = len(train_dataset_worker1)\n",
    "len_of_testing_samples = len(test_dataset_worker1)\n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "   train_dataset_worker1,\n",
    "    batch_size=33,\n",
    "    shuffle=True,\n",
    "    num_workers=2)\n",
    "\n",
    "test_loader = data.DataLoader(\n",
    "    test_dataset_worker1,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1)\n",
    "#======================================= End of istribution and loading of the datasets =======================================#\n",
    "\n",
    "\n",
    "\n",
    "#=================================Start of model creation and distribute workers tasks ========================================#\n",
    "#model = Net()\n",
    "model = CNNCifar(10)\n",
    "#model  = VGG('VGG16')\n",
    "#model = fixup_resnet20()\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay= 1e-6, momentum = 0.9, nesterov = True)\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "device = torch.device(\"cpu\") # or 'gpu'\n",
    "losses = []\n",
    "accuracies = []\n",
    "best_accuracy = 0.0\n",
    "\n",
    "#start training\n",
    "print(\"=======> Start training......\")\n",
    "for epoch in range(0, 10):\n",
    "    losses.extend(train(model, device, train_loader, optimizer, epoch))\n",
    "    currenet_accuracy = test(model, device, test_loader)\n",
    "    accuracies.append(currenet_accuracy)\n",
    "    \n",
    "    # Save the new model just if there is an accuracy improvement\n",
    "    if currenet_accuracy > best_accuracy:\n",
    "        best_accuracy = currenet_accuracy\n",
    "        torch.save(model.state_dict(), os.path.join(PATH, '{}_cifar_net.pth'.format(epoch+1)))\n",
    "    #if epoch > 15:\n",
    "        #for param_group in optimizer.param_groups:\n",
    "            #param_group['lr'] *= 0.97\n",
    "#=================================End of model creation and distribute workers tasks ========================================#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(li): return sum(li)/len(li)\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.xlabel('training batch')\n",
    "plt.ylabel('loss')\n",
    "plt.plot([mean(losses[i:i+1000]) for i in range(len(losses))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17, 7))\n",
    "plt.xticks(range(len(accuracies)))\n",
    "plt.xlabel('training epoch')\n",
    "plt.ylabel('train accuracy')\n",
    "plt.plot(accuracies, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #classes = ('plane', 'car',  'bird',  'cat',  'deer',\n",
    "  #         'dog',   'frog', 'horse', 'ship', 'truck')\n",
    "def test_label_predictions(model, device, test_loader):\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data=data.float()\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            actuals.extend(target.view_as(prediction))\n",
    "            predictions.extend(prediction)\n",
    "    return [i.item() for i in actuals], [i.item() for i in predictions]\n",
    "\n",
    "\n",
    "actuals, predictions = test_label_predictions(model, device, test_loader)\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(actuals, predictions))\n",
    "print('F1 score: %f' % f1_score(actuals, predictions, average='micro'))\n",
    "print('Accuracy score: %f' % accuracy_score(actuals, predictions))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "ax.matshow(confusion_matrix(actuals, predictions), aspect='auto', vmin=0, vmax=1000, cmap=plt.get_cmap('Blues'))\n",
    "plt.ylabel('Actual Category')\n",
    "plt.yticks(range(10), classes)\n",
    "plt.xlabel('Predicted Category')\n",
    "plt.xticks(range(10), classes)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('{0:10s} - {1}'.format('Category','Accuracy'))\n",
    "for i, r in enumerate(confusion_matrix(actuals, predictions)):\n",
    "    print('{0:10s} - {1:.1f}'.format(classes[i], r[i]/np.sum(r)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_class_probabilities(model, device, test_loader, which_class):\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    probabilities = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            #data = data.transpose(3, 1)\n",
    "            data=data.float()\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            actuals.extend(target.view_as(prediction) == which_class)\n",
    "            probabilities.extend(np.exp(output[:, which_class]))\n",
    "    return [i.item() for i in actuals], [i.item() for i in probabilities]\n",
    "\n",
    "which_class = 1\n",
    "actuals, class_probabilities = test_class_probabilities(model, device, test_loader, which_class)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(actuals, class_probabilities)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC for label=plane(%d) class' % which_class)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print model trainable parameters\n",
    "\n",
    "total = 0\n",
    "print('\\nTrainable parameters:')\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, '\\t', param.numel())\n",
    "        total += param.numel()\n",
    "print()\n",
    "print('Total', '\\t', total, \"trainable parametsers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
